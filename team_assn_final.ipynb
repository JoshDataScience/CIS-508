{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaEcDsnzhxV1"
   },
   "source": [
    "# Import Packages & Load Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "B2xKJJc_p6sf"
   },
   "outputs": [],
   "source": [
    "# importing pandas and numpy for data related operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# importing matplotlib and seaborn for visualizations\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import rcParams\n",
    "%matplotlib inline\n",
    "import seaborn as sns \n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# importing packages for preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# importing packages to build and test the model  \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# importing model packages\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# importing packages for stacking\n",
    "from mlxtend.regressor import StackingCVRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9HsfkWTNoHT0"
   },
   "outputs": [],
   "source": [
    "# importing required data files - given training and test_data\n",
    "train_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/TeamAssn/train.csv')\n",
    "test_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/TeamAssn/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmu2FqEpuHEj"
   },
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "id": "Q04K3bXBuKcK",
    "outputId": "7916e825-52f5-4b2f-8ae3-524e05e412df"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Open Date</th>\n",
       "      <th>City</th>\n",
       "      <th>City Group</th>\n",
       "      <th>Type</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>P6</th>\n",
       "      <th>P7</th>\n",
       "      <th>P8</th>\n",
       "      <th>P9</th>\n",
       "      <th>P10</th>\n",
       "      <th>P11</th>\n",
       "      <th>P12</th>\n",
       "      <th>P13</th>\n",
       "      <th>P14</th>\n",
       "      <th>P15</th>\n",
       "      <th>P16</th>\n",
       "      <th>P17</th>\n",
       "      <th>P18</th>\n",
       "      <th>P19</th>\n",
       "      <th>P20</th>\n",
       "      <th>P21</th>\n",
       "      <th>P22</th>\n",
       "      <th>P23</th>\n",
       "      <th>P24</th>\n",
       "      <th>P25</th>\n",
       "      <th>P26</th>\n",
       "      <th>P27</th>\n",
       "      <th>P28</th>\n",
       "      <th>P29</th>\n",
       "      <th>P30</th>\n",
       "      <th>P31</th>\n",
       "      <th>P32</th>\n",
       "      <th>P33</th>\n",
       "      <th>P34</th>\n",
       "      <th>P35</th>\n",
       "      <th>P36</th>\n",
       "      <th>P37</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>07/17/1999</td>\n",
       "      <td>İstanbul</td>\n",
       "      <td>Big Cities</td>\n",
       "      <td>IL</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5653753.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>02/14/2008</td>\n",
       "      <td>Ankara</td>\n",
       "      <td>Big Cities</td>\n",
       "      <td>FC</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6923131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>03/09/2013</td>\n",
       "      <td>Diyarbakır</td>\n",
       "      <td>Other</td>\n",
       "      <td>IL</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2055379.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>02/02/2012</td>\n",
       "      <td>Tokat</td>\n",
       "      <td>Other</td>\n",
       "      <td>IL</td>\n",
       "      <td>6</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>7.5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>2675511.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>05/09/2009</td>\n",
       "      <td>Gaziantep</td>\n",
       "      <td>Other</td>\n",
       "      <td>IL</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4316715.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   Open Date        City  City Group Type  ...  P34  P35  P36  P37    revenue\n",
       "0   0  07/17/1999    İstanbul  Big Cities   IL  ...    5    4    3    4  5653753.0\n",
       "1   1  02/14/2008      Ankara  Big Cities   FC  ...    0    0    0    0  6923131.0\n",
       "2   2  03/09/2013  Diyarbakır       Other   IL  ...    0    0    0    0  2055379.0\n",
       "3   3  02/02/2012       Tokat       Other   IL  ...   18   12   12    6  2675511.0\n",
       "4   4  05/09/2009   Gaziantep       Other   IL  ...    3    4    3    3  4316715.0\n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking first few rows of the training data\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PL0zP3aDuKZK",
    "outputId": "b2d57824-b9ba-499f-ee17-2187fdc2351b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 43)\n",
      "(100000, 42)\n"
     ]
    }
   ],
   "source": [
    "# finding the shape of training and test data files\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3RcePx_0uKXc",
    "outputId": "c3a534d5-bbbd-4a43-e3a9-40d8084c6a89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 137 entries, 0 to 136\n",
      "Data columns (total 43 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Id          137 non-null    int64  \n",
      " 1   Open Date   137 non-null    object \n",
      " 2   City        137 non-null    object \n",
      " 3   City Group  137 non-null    object \n",
      " 4   Type        137 non-null    object \n",
      " 5   P1          137 non-null    int64  \n",
      " 6   P2          137 non-null    float64\n",
      " 7   P3          137 non-null    float64\n",
      " 8   P4          137 non-null    float64\n",
      " 9   P5          137 non-null    int64  \n",
      " 10  P6          137 non-null    int64  \n",
      " 11  P7          137 non-null    int64  \n",
      " 12  P8          137 non-null    int64  \n",
      " 13  P9          137 non-null    int64  \n",
      " 14  P10         137 non-null    int64  \n",
      " 15  P11         137 non-null    int64  \n",
      " 16  P12         137 non-null    int64  \n",
      " 17  P13         137 non-null    float64\n",
      " 18  P14         137 non-null    int64  \n",
      " 19  P15         137 non-null    int64  \n",
      " 20  P16         137 non-null    int64  \n",
      " 21  P17         137 non-null    int64  \n",
      " 22  P18         137 non-null    int64  \n",
      " 23  P19         137 non-null    int64  \n",
      " 24  P20         137 non-null    int64  \n",
      " 25  P21         137 non-null    int64  \n",
      " 26  P22         137 non-null    int64  \n",
      " 27  P23         137 non-null    int64  \n",
      " 28  P24         137 non-null    int64  \n",
      " 29  P25         137 non-null    int64  \n",
      " 30  P26         137 non-null    float64\n",
      " 31  P27         137 non-null    float64\n",
      " 32  P28         137 non-null    float64\n",
      " 33  P29         137 non-null    float64\n",
      " 34  P30         137 non-null    int64  \n",
      " 35  P31         137 non-null    int64  \n",
      " 36  P32         137 non-null    int64  \n",
      " 37  P33         137 non-null    int64  \n",
      " 38  P34         137 non-null    int64  \n",
      " 39  P35         137 non-null    int64  \n",
      " 40  P36         137 non-null    int64  \n",
      " 41  P37         137 non-null    int64  \n",
      " 42  revenue     137 non-null    float64\n",
      "dtypes: float64(9), int64(30), object(4)\n",
      "memory usage: 46.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# understanding data types and null value counts for all columns of the data\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "X9_JDBExuT5P",
    "outputId": "2cabf927-c1ba-4e32-ae94-963cc63231f9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>P6</th>\n",
       "      <th>P7</th>\n",
       "      <th>P8</th>\n",
       "      <th>P9</th>\n",
       "      <th>P10</th>\n",
       "      <th>P11</th>\n",
       "      <th>P12</th>\n",
       "      <th>P13</th>\n",
       "      <th>P14</th>\n",
       "      <th>P15</th>\n",
       "      <th>P16</th>\n",
       "      <th>P17</th>\n",
       "      <th>P18</th>\n",
       "      <th>P19</th>\n",
       "      <th>P20</th>\n",
       "      <th>P21</th>\n",
       "      <th>P22</th>\n",
       "      <th>P23</th>\n",
       "      <th>P24</th>\n",
       "      <th>P25</th>\n",
       "      <th>P26</th>\n",
       "      <th>P27</th>\n",
       "      <th>P28</th>\n",
       "      <th>P29</th>\n",
       "      <th>P30</th>\n",
       "      <th>P31</th>\n",
       "      <th>P32</th>\n",
       "      <th>P33</th>\n",
       "      <th>P34</th>\n",
       "      <th>P35</th>\n",
       "      <th>P36</th>\n",
       "      <th>P37</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>1.370000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>68.000000</td>\n",
       "      <td>4.014599</td>\n",
       "      <td>4.408759</td>\n",
       "      <td>4.317518</td>\n",
       "      <td>4.372263</td>\n",
       "      <td>2.007299</td>\n",
       "      <td>3.357664</td>\n",
       "      <td>5.423358</td>\n",
       "      <td>5.153285</td>\n",
       "      <td>5.445255</td>\n",
       "      <td>5.489051</td>\n",
       "      <td>3.262774</td>\n",
       "      <td>5.299270</td>\n",
       "      <td>5.080292</td>\n",
       "      <td>1.416058</td>\n",
       "      <td>1.386861</td>\n",
       "      <td>1.941606</td>\n",
       "      <td>1.036496</td>\n",
       "      <td>1.941606</td>\n",
       "      <td>4.905109</td>\n",
       "      <td>4.547445</td>\n",
       "      <td>2.270073</td>\n",
       "      <td>2.226277</td>\n",
       "      <td>3.423358</td>\n",
       "      <td>1.372263</td>\n",
       "      <td>1.211679</td>\n",
       "      <td>1.470803</td>\n",
       "      <td>1.145985</td>\n",
       "      <td>3.222628</td>\n",
       "      <td>3.135036</td>\n",
       "      <td>2.729927</td>\n",
       "      <td>1.941606</td>\n",
       "      <td>2.525547</td>\n",
       "      <td>1.138686</td>\n",
       "      <td>2.489051</td>\n",
       "      <td>2.029197</td>\n",
       "      <td>2.211679</td>\n",
       "      <td>1.116788</td>\n",
       "      <td>4.453533e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>39.692569</td>\n",
       "      <td>2.910391</td>\n",
       "      <td>1.514900</td>\n",
       "      <td>1.032337</td>\n",
       "      <td>1.016462</td>\n",
       "      <td>1.209620</td>\n",
       "      <td>2.134235</td>\n",
       "      <td>2.296809</td>\n",
       "      <td>1.858567</td>\n",
       "      <td>1.834793</td>\n",
       "      <td>1.847561</td>\n",
       "      <td>1.910767</td>\n",
       "      <td>1.941668</td>\n",
       "      <td>1.036527</td>\n",
       "      <td>2.729583</td>\n",
       "      <td>2.398677</td>\n",
       "      <td>3.505807</td>\n",
       "      <td>2.030679</td>\n",
       "      <td>3.300549</td>\n",
       "      <td>5.604467</td>\n",
       "      <td>3.708041</td>\n",
       "      <td>2.052630</td>\n",
       "      <td>1.230690</td>\n",
       "      <td>4.559609</td>\n",
       "      <td>2.304112</td>\n",
       "      <td>2.133179</td>\n",
       "      <td>2.612024</td>\n",
       "      <td>2.067039</td>\n",
       "      <td>2.308806</td>\n",
       "      <td>1.680887</td>\n",
       "      <td>5.536647</td>\n",
       "      <td>3.512093</td>\n",
       "      <td>5.230117</td>\n",
       "      <td>1.698540</td>\n",
       "      <td>5.165093</td>\n",
       "      <td>3.436272</td>\n",
       "      <td>4.168211</td>\n",
       "      <td>1.790768</td>\n",
       "      <td>2.576072e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.149870e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.999068e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>68.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.939804e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>102.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.166635e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>136.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.969694e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id          P1          P2  ...         P36         P37       revenue\n",
       "count  137.000000  137.000000  137.000000  ...  137.000000  137.000000  1.370000e+02\n",
       "mean    68.000000    4.014599    4.408759  ...    2.211679    1.116788  4.453533e+06\n",
       "std     39.692569    2.910391    1.514900  ...    4.168211    1.790768  2.576072e+06\n",
       "min      0.000000    1.000000    1.000000  ...    0.000000    0.000000  1.149870e+06\n",
       "25%     34.000000    2.000000    4.000000  ...    0.000000    0.000000  2.999068e+06\n",
       "50%     68.000000    3.000000    5.000000  ...    0.000000    0.000000  3.939804e+06\n",
       "75%    102.000000    4.000000    5.000000  ...    3.000000    2.000000  5.166635e+06\n",
       "max    136.000000   12.000000    7.500000  ...   20.000000    8.000000  1.969694e+07\n",
       "\n",
       "[8 rows x 39 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understanding important statistical parameters for the data \n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q6YWPbHuuT3H",
    "outputId": "9510ace3-8bdc-441c-a249-6227b2c8552c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# checking if any columns in the train and test data have null values\n",
    "print(train_data.isna().sum()[train_data.isna().sum()!= 0])\n",
    "print(test_data.isna().sum()[test_data.isna().sum()!= 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zRGhZhipvc4T",
    "outputId": "3f1d9b2e-32e6-4b34-d7e5-a9caee307edb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "revenue    1.000000\n",
       "P2         0.191518\n",
       "P28        0.155534\n",
       "P6         0.139094\n",
       "P29        0.114846\n",
       "P13        0.105085\n",
       "P21        0.097411\n",
       "P11        0.084247\n",
       "P8         0.084215\n",
       "P22        0.083562\n",
       "P10        0.073220\n",
       "P34        0.072343\n",
       "P1         0.070217\n",
       "P17        0.067137\n",
       "P30        0.066203\n",
       "P32        0.065857\n",
       "P12        0.062193\n",
       "P7         0.051165\n",
       "P36        0.050534\n",
       "P9         0.050352\n",
       "P35        0.050156\n",
       "P23        0.045507\n",
       "P31        0.040418\n",
       "P16        0.037997\n",
       "P25        0.036365\n",
       "P4         0.035685\n",
       "P18        0.034537\n",
       "P33        0.032426\n",
       "P5         0.028191\n",
       "P19        0.027984\n",
       "P3         0.024613\n",
       "P37        0.019051\n",
       "P24        0.014222\n",
       "P20        0.014147\n",
       "P27        0.013680\n",
       "P26        0.007650\n",
       "P14        0.006441\n",
       "Id         0.006277\n",
       "P15        0.000742\n",
       "Name: revenue, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understanding the correlation between target variable and predictors\n",
    "abs(train_data.corr()['revenue']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "N8le6PPduX53",
    "outputId": "6cb742b1-0445-4a48-8718-0e183ff74ca8"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# box plot for column P1\n",
    "sns.boxplot(x=train_data[\"P1\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "IYeuKgFpuX36",
    "outputId": "3781c4d6-69c5-48fb-a170-5150f47c811c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# box plot for column P5\n",
    "sns.boxplot(x=train_data[\"P5\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "UoeOClCpu9pH",
    "outputId": "86f36790-71d0-4b3d-da72-672dbc1758b3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEMCAYAAAAvaXplAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXNElEQVR4nO3dfZQldX3n8Xf3TM8EmYHFThMiIqLC110Ww8Oy+Iw5ieQRFxEDuEA2JlnUHGHdDeGIgqy7usiwURBcMCQGUThZdDMS10jgnECYZTmriRNFj1+JiowgOjbIMIrz1L1/VDVzp6sfbt2pW/f29Pt1Tp/pW1W/qm/XVPfn1sP9/Uamp6eRJKnT6KALkCQNH8NBklRhOEiSKgwHSVKF4SBJqlg56AL20mrgROB7wK4B1yJJS8UK4OeBLwDb5lpgqYfDicC9gy5CkpaoVwEb5pqx1MPhewBPPPFjpqaG5/Ma4+NrmJzcOugyFmSNzbDGZlhjM7qtcXR0hIMO2h/Kv6FzWerhsAtgamp6qMIBGLp65mKNzbDGZlhjM2rWOO/leG9IS5IqDAdJUoXhIEmqMBwkSRWGgySpwnCQJFUYDpKkiqX+OYd92gEH7sfqVfX+i7Zt38mWJ5/uU0WSlgvDYYitXrWSi66+p1abdRee3KdqJC0nXlaSJFUYDpKkCsNBklRhOEiSKgwHSVKF4SBJqjAcJEkVhoMkqcJwkCRVGA6SpArDQZJUYThIkioMB0lSRWu9skbEeuAIYArYCrw9MzdGxFHATcA4MAmcl5kPtlWXJKmqzTOH387MX8jM44CrgD8rp18PXJeZRwHXATe0WJMkaQ6thUNmPtnx8kBgKiIOBo4Hbi2n3wocHxETbdUlSapqdbCfiLgROAUYAX4VOAx4JDN3AWTmroh4tJy+uc3aJEm7tRoOmfl7ABFxLrAOuLSJ9Y6Pr2liNY2amFjbyHrGxur/F3W77aZq7CdrbIY1NmM51TiQYUIz8+aI+CjwXeDQiFhRnjWsAJ4DbKqzvsnJrUxNTfej1J5MTKxl8+anGlnPjh07a7frZttN1dhP1tgMa2zGvlTj6OjIom+qW7nnEBFrIuKwjtenAo8DPwA2AmeXs84GvpSZXlKSpAFq68xhf+C2iNgf2EURDKdm5nREvAW4KSIuA54AzmupJknSPFoJh8z8PvDSeeZ9HTipjTokSd3xE9KSpArDQZJUYThIkioMB0lSheEgSaowHCRJFYaDJKnCcJAkVRgOkqQKw0GSVGE4SJIqDAdJUoXhIEmqMBwkSRWGgySpwnCQJFUYDpKkCsNBklRhOEiSKgwHSVKF4SBJqjAcJEkVhoMkqWJlGxuJiHHgZuCFwHbgQeD8zNwcEdPAV4CpcvFzM/MrbdQlSZpbK+EATANXZubdABGxDrgC+N1y/sszc2tLtUiSFtFKOGTm48DdHZPuB97axrYlSfW1debwjIgYpQiG2zsm3x0RK4G/Bi7PzG111jk+vqbBCpsxMbG2kfWMjdX/L+p2203V2E/W2AxrbMZyqrH1cAA+DGwFri1fPy8zN0XEART3JS4F3l1nhZOTW5mamm62yr0wMbGWzZufamQ9O3bsrN2um203VWM/WWMzrLEZ+1KNo6Mji76pbvVppYi4CjgSODMzpwAyc1P57xbgRuAVbdYkSapqLRwi4v3ACcBpM5eNIuKgiNiv/H4lcAawsa2aJElza+tR1qOBdwLfAO6LCIBvA1cCN5SPs44B91FcVpIkDVBbTyt9FRiZZ/ZL2qhBktQ9PyEtSaowHCRJFYaDJKnCcJAkVRgOkqQKw0GSVGE4SJIqDAdJUoXhIEmqMBwkSRWGgySpwnCQJFUYDpKkikGMBLekHXDgfqxetfhu6xyqb8fOXYytXNHPsiSpUYZDTatXreSiq+9ZcJmxsZV7DO+57sKTF20zl3UXnly7jSQ1wctKkqQKw0GSVGE4SJIqDAdJUoXhIEmqMBwkSRWGgySpopXPOUTEOHAz8EJgO/AgcH5mbo6IlwI3APsBDwHnZOYP2qhLkjS3ts4cpoErMzMy8xjgm8AVETEKfAL4g8w8Cvg74IqWapIkzaOVcMjMxzPz7o5J9wOHAycAP83MDeX064HfaqMmSdL8Wr/nUJ4tvBW4HXge8J2ZeZn5Q2A0Ip7ddl2SpN0G0bfSh4GtwLXA65tY4fj4miZW07WxscV32+xlumnT67Zm6+z0r4nlBskam2GNzVhONXb9lyci3piZt80x/YzM/FSX67gKOBI4NTOnIuJhistLM/N/FpjKzMe7rQtgcnIrU1PTdZr0bGJi7R6d6s1ldsd7wKJt5tNLu82bn1p0mYmJtV0tN0jW2AxrbMa+VOPo6Miib6rrXFb603mmf7SbxhHxfop7DKdl5rZy8t8D+0XEK8vXbwEqASRJateiZw4R8YLy29GIOAIY6Zj9AuCnXazjaOCdwDeA+yIC4NuZ+fqIOBe4ISJ+hvJR1lo/gSSpcd1cVvonikdRRygeQe30GHD5YivIzK+yZ6h0zrsPOKaLOiRJLVk0HDJzFCAi7slMR5+RpGWg63sOBoMkLR91nlY6AngfcCywx23uzHxew3VJkgaozkP0t1Dcc/hPwE/6U44kaRjUCYejgVdk5lS/ipEkDYc6n3P4O+C4fhUiSRoedc4cHgI+HxF/SfEI6zMy87Imi5IkDVadcNgf+CwwBhzWn3IkScOg63DIzN/pZyGSpOFR51HWF8w3LzO/1Uw5kqRhUOeyUmc3GjNmukJd0VhFkqSBq3NZaY8nmyLiEOA9wL1NFyVJGqyeR4LLzMeA/wD8t+bKkSQNg70dJjSAZzVRiCRpeNS5IX0vu+8xQBEKRwPvbboo9W7Hzqnaw4Ru276TLU8+3c+yJC0xdW5I3zjr9Y+Bf8zMBxusR3tpbOUoF119z+LLdQxluu5CO9yVtKc6N6Rv6mchkqThUeey0hjwbuBc4DnAo8DNwPsyc3t/ypMkDUKdy0pXAv8aeAvwHeBw4FLgAOAdzZcmSRqUOuHwRuAXMnOyfJ0R8Q/AP2I4SNI+pc6jrCM1p0uSlqg6Zw63AX8VEf8ZeJjistK7y+mSpH1InXD4I4owuI7ihvQjwK3Af+1DXZKkAVo0HCLiFcDrMvNi4LLya2beB4Djgfu7WM9VwBuA5wPHZOYD5fSHgJ+WXwAXZ+YddX4ISVKzujlzuAT4yDzz/hZ4F3BqF+tZD1zN3B31nTETFpKkwevmhvSxwOfnmXcXcEI3G8rMDZm5qdvCJEmD082ZwwHAKmCuznfGgO468lnYJyNiBNgAXJKZP6rTeHx8TQMldG9sbPHdNnuZbtr0uq1e23Qu121/TG0b1ro6WWMzrLEZTdXYzV+RrwOnAJ+ZY94p5fy98arM3BQRq4EPAdcC59RZweTkVqamphdfsAETE2uf6ZNoPp39Fs1YrM18emnXTZvZNW7e/FTt7fTbxMTaoayrkzU2wxqb0W2No6Mji76p7uay0geBGyLi9IgYBYiI0Yg4Hbge+OMu1jGvmUtNmbmN4t7GK/ZmfZKkvbfomUNm3lKO+nYTsDoifgj8LLANeE9m3trrxiNif2BlZj5ZXlY6C9jY6/okSc3o6uJ0Zv5xRNwIvAwYByaB/5uZW7rdUERcA5wOHALcFRGTFE85fToiVlCMQ/014G31fgRJUtPqdNm9Bej58weZeQFwwRyzjut1nZKk/tjbYUIlSfsgw0GSVGE4SJIqDAdJUoXhIEmqMBwkSRWGgySpwnCQJFUYDpKkCsNBklRhOEiSKgwHSVKF4SBJquht7ErtU3bsnKo9tOC27TvZ8uRcI8dK2hcYDmJs5SgXXX1PrTbrLjy5T9VIGgZeVpIkVRgOkqQKw0GSVGE4SJIqDAdJUoXhIEmqMBwkSRWtfM4hIq4C3gA8HzgmMx8opx8F3ASMA5PAeZn5YBs1SZLm19aZw3rg1cB3Zk2/HrguM48CrgNuaKkeSdICWgmHzNyQmZs6p0XEwcDxwK3lpFuB4yNioo2aJEnzG+Q9h8OARzJzF0D576PldEnSAO0TfSuNj69pdXtjY4vvttnLdNOm12312qZzuV62U7ezvl60sY29ZY3NsMZmNFXjIMNhE3BoRKzIzF0RsQJ4Tjm9lsnJrUxNTTde4FwmJtayY8fOBZcZG1tZWWaxNvPppV03bWbX2Mt2Nm9+qnabOiYm1vZ9G3vLGpthjc3otsbR0ZFF31QP7LJSZv4A2AicXU46G/hSZm4eVE2SpEIr4RAR10TEd4HnAndFxFfLWW8B3h4R3wDeXr6WJA1YK5eVMvMC4II5pn8dOKmNGiRJ3fMT0pKkin3iaaVeHXDgfqxetax3gSTNaVn/ZVy9aqXDY0rSHLysJEmqMBwkSRWGgySpwnCQJFUYDpKkCsNBklRhOEiSKgwHSVKF4SBJqjAcJEkVy7r7DLWvbn9WExNr2bZ9J1uefLqPVUmazXBQq+r0ZzUzWp39WUnt87KSJKnCcJAkVRgOkqQKw0GSVGE4SJIqDAdJUoXhIEmqGIrPOUTEQ8BPyy+AizPzjoEVJEnL3FCEQ+mMzHxg0EVIkrysJEmawzCdOXwyIkaADcAlmfmjQRckScvVsITDqzJzU0SsBj4EXAuc023j8fE1PW94bKz+LuimzexletlOr+26bdO5XN3t7Ng5xcTE2lptetnWzLK9bqsNw1zbDGtsxnKqcSjCITM3lf9ui4iPALfXaT85uZWpqena252YWMuOHTtrt1uszUyHcXXa9LqtXtvMrrHudsZWjnbdgV6ndRee3PW2OmvcvPmp2ttqw8TE2qGtbYY1NmNfqnF0dGTRN9UDv+cQEftHxIHl9yPAWcDGwVYlScvbMJw5/Bzw6YhYAawAvga8bbAlSdLyNvBwyMxvAccNug5J0m4Dv6wkSRo+hoMkqcJwkCRVGA6SpArDQZJUYThIkioMB0lSxcA/5yD1wwEH7sfqVfUO723bd7Llyaf7VJG0tBgO2ietXrWydt9P6y48uU/VSEuPl5UkSRWGgySpwnCQJFUYDpKkCsNBklTh00oaenszJGm/t7Nj51SfqpEGy3DQ0OtlSNJeHkttazvSUuBlJUlSheEgSaowHCRJFYaDJKnCG9KS9tBLp4Uw3B0XDvvPNIwdRRoOkvbQS6eFMNxPbg37zzSMHUV6WUmSVDEUZw4RcRRwEzAOTALnZeaDg61KkpavYTlzuB64LjOPAq4DbhhwPZK0rA38zCEiDgaOB15bTroVuDYiJjJz8yLNVwCMjo70vP2D1q5uvM3KsZXs3LFir7fTa7tu2syusV/b2Zt2nTW2VV8vbfbm+GtL3Rp7/b/dm33R7/3YxM/UzxqbOva6qbFjmRXzLTMyPT1du6AmRcQJwMcz8+iOaV8DzsnMf1ik+SuBe/tZnyTtw14FbJhrxsDPHPbSFyh+uO8BuwZciyQtFSuAn6f4GzqnYQiHTcChEbEiM3dFxArgOeX0xWxjntSTJC3omwvNHPgN6cz8AbAROLucdDbwpS7uN0iS+mTg9xwAIuLFFI+yHgQ8QfEoaw62KklavoYiHCRJw2Xgl5UkScPHcJAkVRgOkqQKw0GSVDEMn3NYMrrpIDAiLgXOovhQ3g7gksy8o5z358AvAz8sF78tM983gBovB94GPFpO+j+Z+QflvGcBHwNOAHYCf5iZnx1AjR8HXtIx6SXAaZl5+0L1N1TfVcAbgOcDx2TmA3MsswK4BvhVYBq4IjNvXGxeyzUO+ljspsbLGeyx2E2NAzsWy+2PAzcDLwS2Aw8C589+3H+h/dXLvvTMoZ5uOgj8f8CJmfkS4M3AX0TEfh3zr8jMY8uvRn8Za9QIRZclM3V0Hsx/CGzJzBcBpwI3RsSatmvMzPNm6gN+m+IR5zu6qL8J64FXA99ZYJl/C7wIOBJ4GXB5RDy/i3lt1jjoY7GbGmGwx+KiNQ74WITiDcaVmRmZeQzFh9eumGO5hfZX7X1pOHSpo4PAW8tJtwLHR8RE53KZeUdm/qR8+WVghOId8tDUuIgzKf9Yl+/mvwj82oBr/F3gk5m5rak6FpKZGzJzsU/onwn8SWZOle/g1gNv7GJeazUO8lgst9/NflxIX4/Fcr11a2z1WATIzMcz8+6OSfcDh8+x6EL7q/a+NBy6dxjwSGbuAij/fbScPp/zgG9m5nc7pv3HiPhKRKyPiH8+wBrPiogvR8TfRMTLOqY/jz3fRT08T/s2aiQiVgFvAv5s1qz56m/LQvup3/uwF20fi3UM6lisZRiOxYgYBd4K3D7H7EaPScOhTyLiZOC/sLtbEIB3AS8qTw3/F/D58vp0264HjigvN6wDPlNe1xxGpwEPZ+bGjmlLqf6B81hszDAcix8GtgLX9nEbgOFQxzMdBMIzNx3n7CCwfPfwCYqbVs90A5KZj2TmVPn9x4E1wHPbrjEzH8vMHeX3d5bz/2U5+2H2PGV93uz2bdTY4c3Meqe2SP1tWWg/9Xsfdm2Ax2JXBnws1jXQY7G8eX4kcObM/90sjR6ThkOXuu0gMCJOBP4COCNnjUcREYd2fP8rFE+RPDKAGjvrOJbiSY2ZPxy3AeeX844ETgQ+33aN5fafS9El+ydr1N+W24Dfj4jR8n7JacCnupjXmkEei90a5LFYx6CPxYh4P8WTRqctcL9jof1Ve1/at1IN83UQGBGfAy7LzC9GxBcoDpDOX7RzM/MrEXEX8HPAFLAFuCgz7x9AjTdRHGi7KB6Ne09mfq5svz/w58Bx5fw/yszPtF1judy7KB4vPGtW+3nrb6i+a4DTgUMoHvWczMyjZ+3DFRSn9qeUzT6QmR8t2887r+UaB30sdlPjoI/FRWsslxvIsVhu42jgAeAbwNPl5G9n5usjYiPw65n56EL7q5d9aThIkiq8rCRJqjAcJEkVhoMkqcJwkCRV2PGeJC0B3XQSOGv5lwMf6Zh0MPBYZh7fzfYMB0laGtYDVwP3drNwZt4HHDvzOiLWAxu63ZjhIElLQGZuAIiIPaZHxEkUvbQeUE66LDP/96xlDqb43M353W7Pew5SKSJ8s6QlJSL+GUX/Tm/KzBOA3wRuKKd3Og/4m8z8frfr9pdBy1pEPAT8D4oxGCIifgm4EvgXFL1YXpiZd0fEmRSfIv5XHW3fAfxiZr4uIlYD7wN+C1gN/CXwjsx8OiJeQ9G/0QeBiyk+oXpJZn6sXM/dwCdy92BB/w74vcx8Zfn6xRQdrp0AbAYuzcz/2a99oiXl5cARwF93nFFMU4wn8sWO5X4HeGedFRsOUtG/029QdCXxZeBcin5nfgn4dPnH+a8oBkg5MnePWvcm4L+X319BMVLXsRSjrt0CXMbuX8hDgAOBQ4HXAp+KiPWZ+cRChZXdHtxZruvXgGOAOyPigcz82t7+4FryRoAvZ+ar51sgIl4KPBuo1a2Hl5UkuKYc8OUc4HOZ+blyoJ47Kd59/Xo5aM5nKDsMLDsvezFwe0SMAP+e4kzh8cx8Cng/xRCdM3YA783MHWXfO1uBPS8ez+03gYcy82OZuTMzvwR8moYHD9KSdR9wZET84syEiDixPCZnvBm4OTN31lmxZw7S7q6LDwfeGBGndswbA/62/P4WijOF91KcNazPzJ+UN/ueBfx9x6n9CNA5PsLkrF/On1B0k72Yw4GTIuJHHdNWUowprGVkVieBd0XETCeBrwPWRcSHgFXAtyiGAp2OYljYM4GT6m7PcJCKa7RQhMTNmfn78yx3JzBRds18NvCOcvoPKXrLPDoze+n2+scU4TLjkI7vNwH3ZOZre1iv9iGZeQFwwRzTvwC8Zp42T1NczqzNy0rSbp8ATo2IX4mIFRHxMxHxmrIvf8pBXW6jGPHr2RRhQTnwyp8AHyzPIoiIQ8txErqxETg9Ip4VES+iGKd4xmeBoyLi3IgYK79OjMEO66llwHCQSuV9h38DXELxVNAm4CL2/D25Bfhl4LZZl4kuBv4JuD8itgB30d09BSieYtoOfJ9inItnBpQp71+cQnH/4lHgMeADFE9ESX3jeA6SpArPHCRJFYaDJKnCcJAkVRgOkqQKw0GSVGE4SJIqDAdJUoXhIEmqMBwkSRX/H6+/5YYjVV1oAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# box plot for column revenue\n",
    "sns.histplot(data=train_data, x=\"revenue\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "xpZNC3LAu9mk",
    "outputId": "5d919bdc-5cd7-417a-ae77-fa0d2a6a3714"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# box plot for column P28\n",
    "sns.histplot(data=train_data, x=\"P28\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "pvNw-jB2u9hM",
    "outputId": "c81d5c6c-e0bc-41b6-d59c-48eb747ef664"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# heat map to indicate correlation\n",
    "sns.heatmap(train_data.corr());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "FndQpt_Wvc6E",
    "outputId": "97ad0ffa-49b7-490a-f9ac-3544f84ca751"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bar plot between Type and Revenue\n",
    "sns.barplot(x=\"Type\", y=\"revenue\", data=train_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qym3W5zBht14"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vqNgmGHqq4Gx"
   },
   "outputs": [],
   "source": [
    "# listing all categorical and numerical columns \n",
    "cat_cols = ['City Group', 'Type']\n",
    "num_cols = ['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8',\n",
    "            'P9', 'P10', 'P11', 'P12', 'P13', 'P14', 'P15', 'P16', 'P17', 'P18',\n",
    "            'P19', 'P20', 'P21', 'P22', 'P23', 'P24', 'P25', 'P26', 'P27', 'P28',\n",
    "            'P29', 'P30', 'P31', 'P32', 'P33', 'P34', 'P35', 'P36', 'P37']\n",
    "target_col = ['revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vfKaCTvhi7yT"
   },
   "outputs": [],
   "source": [
    "# making a copy of train data and removing unwanted columns\n",
    "df = train_data.copy()\n",
    "df.drop(['Id', 'Open Date', 'City'], inplace=True, axis=1)\n",
    "\n",
    "# splitting training data into set of predictors and target variable\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.revenue.copy()\n",
    "\n",
    "# performing imputation to tackle missing values for categorical columns\n",
    "cat_imputer = SimpleImputer(strategy ='most_frequent')\n",
    "cat_imputer = cat_imputer.fit(X[cat_cols]) \n",
    "X[cat_cols] = cat_imputer.transform(X[cat_cols])\n",
    "\n",
    "# performing imputation to tackle missing values for numerical columns\n",
    "num_imputer = SimpleImputer(strategy ='mean')\n",
    "num_imputer = num_imputer.fit(X[num_cols]) \n",
    "X[num_cols] = num_imputer.transform(X[num_cols])\n",
    "\n",
    "# encoding categorical columns to create dummy variables\n",
    "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "X['Type_MB'] = 0\n",
    "\n",
    "# clipping outliers\n",
    "X[num_cols] = X[num_cols].apply(lambda x: x.clip(*x.quantile([0.05, 0.95])))\n",
    "\n",
    "# normalizing the dataset\n",
    "scaler = StandardScaler()\n",
    "X[num_cols] = scaler.fit_transform(X[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eYCPWvbncyPs"
   },
   "outputs": [],
   "source": [
    "# splitting given dataset into training and test data (test data created here is used for model validation and asessment)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9F9kx__Y0Yp"
   },
   "source": [
    "# Individual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TEnQZEDNu2P4",
    "outputId": "440fe738-66d6-419e-db6a-b958adb1adec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE - Default Model: 3404146.954168327\n",
      "Best Score:  -2345328.3616909925\n",
      "Best Parameters: {'splitter': 'best', 'min_weight_fraction_leaf': 0.1, 'criterion': 'friedman_mse'}\n",
      "RMSE - Tuned Model: 3537986.103315851\n"
     ]
    }
   ],
   "source": [
    "# model with default parameters\n",
    "reg = DecisionTreeRegressor(random_state=0)\n",
    "\n",
    "# training the model on split data, and predicting outcomes for validation set \n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# generating rmse\n",
    "print('RMSE - Default Model:', mean_squared_error(y_test, y_pred, squared=False))\n",
    "\n",
    "# hyperparameter tuning the regressor\n",
    "params = {\"criterion\": [\"mse\", 'friedman_mse'],\n",
    "          \"splitter\": ['best', 'random'], \n",
    "          \"min_weight_fraction_leaf\": [0.01, 0.05, 0.1]}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "grid = RandomizedSearchCV(estimator = reg, param_distributions = params, cv = 3, scoring = \"neg_root_mean_squared_error\")\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Parameters and corresponding scores for random search\n",
    "print(\"Best Score: \", grid.best_score_)\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "\n",
    "# Rebuilding the model with obtained parameters\n",
    "reg = DecisionTreeRegressor(splitter='best', min_weight_fraction_leaf=0.1, criterion='friedman_mse', random_state=0)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# generating rmse for final model\n",
    "print('RMSE - Tuned Model:', mean_squared_error(y_test, y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ITb8cerxu2Hi",
    "outputId": "88ee1681-5391-48b7-9601-436deef7d956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE - Default Model: 3113472.4549005986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  -2352376.541117786\n",
      "Best Parameters: {'n_estimators': 100, 'min_weight_fraction_leaf': 0.05, 'criterion': 'mse'}\n",
      "RMSE - Tuned Model: 3175531.1131034787\n"
     ]
    }
   ],
   "source": [
    "# model with default parameters\n",
    "reg = RandomForestRegressor(random_state=0)\n",
    "\n",
    "# training the model on split data, and predicting outcomes for validation set \n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# generating rmse\n",
    "print('RMSE - Default Model:', mean_squared_error(y_test, y_pred, squared=False))\n",
    "\n",
    "# hyperparameter tuning the regressor\n",
    "params = {\"criterion\": [\"mse\"],\n",
    "          \"min_weight_fraction_leaf\": [0.01, 0.05, 0.1],\n",
    "          \"n_estimators\": [100, 200, 300]}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "grid = RandomizedSearchCV(estimator = reg, param_distributions = params, cv = 3, scoring = \"neg_root_mean_squared_error\")\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Parameters and corresponding scores for random search\n",
    "print(\"Best Score: \", grid.best_score_)\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "\n",
    "# Rebuilding the model with obtained parameters\n",
    "reg = RandomForestRegressor(n_estimators=100, min_weight_fraction_leaf=0.05, criterion='mse', random_state=0)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# generating rmse for final model\n",
    "print('RMSE - Tuned Model:', mean_squared_error(y_test, y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9viIGwaajn2",
    "outputId": "7bc7586e-26fe-44ac-a54d-afda46383a6a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE - Default Model: 5950411.574775258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  -4879485.19441012\n",
      "Best Parameters: {'learning_rate': 'constant', 'hidden_layer_sizes': (200,), 'activation': 'relu'}\n",
      "RMSE - Tuned Model: 5950359.743679128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# model with default parameters\n",
    "reg = MLPRegressor(random_state=0)\n",
    "\n",
    "# training the model on split data, and predicting outcomes for validation set \n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# generating rmse\n",
    "print('RMSE - Default Model:', mean_squared_error(y_test, y_pred, squared=False))\n",
    "\n",
    "# hyperparameter tuning the regressor\n",
    "params = {\"hidden_layer_sizes\": [(100, ), (200, )],\n",
    "          \"activation\": ['logistic', 'relu', 'tanh'], \n",
    "          \"learning_rate\": ['constant', 'adaptive']}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "grid = RandomizedSearchCV(estimator = reg, param_distributions = params, cv = 3, scoring = \"neg_root_mean_squared_error\")\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Parameters and corresponding scores for random search\n",
    "print(\"Best Score: \", grid.best_score_)\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "\n",
    "# Rebuilding the model with obtained parameters\n",
    "reg = MLPRegressor(learning_rate='adaptive', hidden_layer_sizes=(200, ), activation='relu', random_state=0)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# generating rmse for final model\n",
    "print('RMSE - Tuned Model:', mean_squared_error(y_test, y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XClAKu0_ajlP",
    "outputId": "172d5f35-a41b-4dbd-bb06-164593fb2b0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3461413.4865653166\n",
      "Best Score:  -2337445.834222009\n",
      "Best Parameters: {'kernel': 'linear', 'C': 1.5}\n",
      "RMSE: 3461476.4410934015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# model with default parameters\n",
    "reg = SVR()\n",
    "\n",
    "# training the model on split data, and predicting outcomes for validation set \n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# generating rmse\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\n",
    "\n",
    "# hyperparameter tuning the regressor\n",
    "params = {\"kernel\": [\"linear\", \"rbf\", 'sigmoid'],\n",
    "          \"C\": [0.5, 1, 1.5]}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "grid = RandomizedSearchCV(estimator = reg, param_distributions = params, cv = 3, scoring = \"neg_root_mean_squared_error\")\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Parameters and corresponding scores for random search\n",
    "print(\"Best Score: \", grid.best_score_)\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "\n",
    "# Rebuilding the model with obtained parameters\n",
    "reg = SVR(kernel='linear', C=1.5)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# generating rmse for final model\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CkactewLajiN",
    "outputId": "e3651ad9-524b-4b6c-8bb4-4cf80166c8fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3259756.9991251137\n",
      "Best Score:  -4879596.489377307\n",
      "Best Parameters: {'penalty': 'l1', 'loss': 'huber'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 2 is smaller than n_iter=10. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 5950468.502483623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# model with default parameters\n",
    "reg = SGDRegressor(random_state=0)\n",
    "\n",
    "# training the model on split data, and predicting outcomes for validation set \n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# generating rmse\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\n",
    "\n",
    "# hyperparameter tuning the regressor\n",
    "params = {\"penalty\": [\"l1\", \"l2\"],\n",
    "          \"loss\": ['huber']}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "grid = RandomizedSearchCV(estimator = reg, param_distributions = params, cv = 3, scoring = \"neg_root_mean_squared_error\")\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Parameters and corresponding scores for random search\n",
    "print(\"Best Score: \", grid.best_score_)\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "\n",
    "# Rebuilding the model with obtained parameters\n",
    "reg = SGDRegressor(penalty='l1', loss='huber', random_state=0)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# generating rmse for final model\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLS3oS1EY4Um"
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "W1RJXnpT9w6D"
   },
   "outputs": [],
   "source": [
    "# setting up the base layer for stacking\n",
    "\n",
    "base_models = [\n",
    "               DecisionTreeRegressor(splitter='best', min_weight_fraction_leaf=0.1, criterion='friedman_mse', random_state=0),\n",
    "               RandomForestRegressor(n_estimators=100, min_weight_fraction_leaf=0.05, criterion='mse', random_state=0),\n",
    "               MLPRegressor(learning_rate='adaptive', hidden_layer_sizes=(200, ), activation='relu', random_state=0, early_stopping=True),\n",
    "               SVR(kernel='linear', C=1.5),\n",
    "               SGDRegressor(penalty='l1', loss='huber', random_state=0)\n",
    "]\n",
    "\n",
    "# setting up the stacking regressor - variant 1 \n",
    "\n",
    "meta_model1 = XGBRegressor()\n",
    "\n",
    "sreg1 = StackingCVRegressor(base_models,\n",
    "                           shuffle = False,\n",
    "                           cv = 3,\n",
    "                           meta_regressor = meta_model1)\n",
    "\n",
    "\n",
    "# setting up the stacking regressor - variant 2\n",
    "\n",
    "meta_model2 = SGDRegressor()\n",
    "\n",
    "sreg2 = StackingCVRegressor(base_models,\n",
    "                           shuffle = False,\n",
    "                           cv = 3,\n",
    "                           meta_regressor = meta_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0CoMKaQ590Hv",
    "outputId": "29bfbc97-ea4b-4f6b-b830-fcdd6b9f0380"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:39:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Model - Variant 1\n",
      "RMSE: 3444645.876121449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Model - Variant 2\n",
      "RMSE: 1.7752498151932912e+22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# training the stacked model - variant 1 \n",
    "sreg1.fit(X_train.values, y_train.values)\n",
    "y_pred1 = sreg1.predict(X_test)\n",
    "\n",
    "# generating rmse\n",
    "print('Stacked Model - Variant 1')\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred1, squared=False))\n",
    "\n",
    "# training the stacked model - variant 2\n",
    "sreg2.fit(X_train.values, y_train.values)\n",
    "y_pred2 = sreg2.predict(X_test)\n",
    "\n",
    "# generating rmse\n",
    "print('Stacked Model - Variant 2')\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred2, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUwUqgcTsxjc",
    "outputId": "b171765d-8326-491d-e4d5-e368d94bae08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] meta_regressor__n_estimators=100, meta_regressor__learning_rate=0.05, meta_regressor__booster=dart \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:33:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta_regressor__n_estimators=100, meta_regressor__learning_rate=0.05, meta_regressor__booster=dart, total=   1.0s\n",
      "[CV] meta_regressor__n_estimators=100, meta_regressor__learning_rate=0.05, meta_regressor__booster=dart \n",
      "[19:33:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  meta_regressor__n_estimators=100, meta_regressor__learning_rate=0.05, meta_regressor__booster=dart, total=   1.0s\n",
      "[CV] meta_regressor__n_estimators=100, meta_regressor__learning_rate=0.05, meta_regressor__booster=dart \n",
      "[19:33:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  meta_regressor__n_estimators=100, meta_regressor__learning_rate=0.05, meta_regressor__booster=dart, total=   1.0s\n",
      "[CV] meta_regressor__n_estimators=300, meta_regressor__learning_rate=0.01, meta_regressor__booster=gbtree \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:33:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta_regressor__n_estimators=300, meta_regressor__learning_rate=0.01, meta_regressor__booster=gbtree, total=   1.0s\n",
      "[CV] meta_regressor__n_estimators=300, meta_regressor__learning_rate=0.01, meta_regressor__booster=gbtree \n",
      "[19:33:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  meta_regressor__n_estimators=300, meta_regressor__learning_rate=0.01, meta_regressor__booster=gbtree, total=   1.0s\n",
      "[CV] meta_regressor__n_estimators=300, meta_regressor__learning_rate=0.01, meta_regressor__booster=gbtree \n",
      "[19:33:44] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  meta_regressor__n_estimators=300, meta_regressor__learning_rate=0.01, meta_regressor__booster=gbtree, total=   1.0s\n",
      "[CV] meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.1, meta_regressor__booster=gbtree \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:33:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.1, meta_regressor__booster=gbtree, total=   1.0s\n",
      "[CV] meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.1, meta_regressor__booster=gbtree \n",
      "[19:33:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.1, meta_regressor__booster=gbtree, total=   1.0s\n",
      "[CV] meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.1, meta_regressor__booster=gbtree \n",
      "[19:33:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.1, meta_regressor__booster=gbtree, total=   1.0s\n",
      "[CV] meta_regressor__n_estimators=100, meta_regressor__learning_rate=0.05, meta_regressor__booster=gblinear \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:33:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta_regressor__n_estimators=100, meta_regressor__learning_rate=0.05, meta_regressor__booster=gblinear, total=   1.0s\n",
      "[CV] meta_regressor__n_estimators=100, meta_regressor__learning_rate=0.05, meta_regressor__booster=gblinear \n",
      "[19:33:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  meta_regressor__n_estimators=100, meta_regressor__learning_rate=0.05, meta_regressor__booster=gblinear, total=   1.0s\n",
      "[CV] meta_regressor__n_estimators=100, meta_regressor__learning_rate=0.05, meta_regressor__booster=gblinear \n",
      "[19:33:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  meta_regressor__n_estimators=100, meta_regressor__learning_rate=0.05, meta_regressor__booster=gblinear, total=   0.9s\n",
      "[CV] meta_regressor__n_estimators=300, meta_regressor__learning_rate=0.1, meta_regressor__booster=dart \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:33:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta_regressor__n_estimators=300, meta_regressor__learning_rate=0.1, meta_regressor__booster=dart, total=   1.1s\n",
      "[CV] meta_regressor__n_estimators=300, meta_regressor__learning_rate=0.1, meta_regressor__booster=dart \n",
      "[19:33:52] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  meta_regressor__n_estimators=300, meta_regressor__learning_rate=0.1, meta_regressor__booster=dart, total=   1.1s\n",
      "[CV] meta_regressor__n_estimators=300, meta_regressor__learning_rate=0.1, meta_regressor__booster=dart \n",
      "[19:33:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  meta_regressor__n_estimators=300, meta_regressor__learning_rate=0.1, meta_regressor__booster=dart, total=   1.1s\n",
      "[CV] meta_regressor__n_estimators=100, meta_regressor__learning_rate=0.1, meta_regressor__booster=gblinear \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:33:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta_regressor__n_estimators=100, meta_regressor__learning_rate=0.1, meta_regressor__booster=gblinear, total=   1.0s\n",
      "[CV] meta_regressor__n_estimators=100, meta_regressor__learning_rate=0.1, meta_regressor__booster=gblinear \n",
      "[19:33:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  meta_regressor__n_estimators=100, meta_regressor__learning_rate=0.1, meta_regressor__booster=gblinear, total=   1.0s\n",
      "[CV] meta_regressor__n_estimators=100, meta_regressor__learning_rate=0.1, meta_regressor__booster=gblinear \n",
      "[19:33:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  meta_regressor__n_estimators=100, meta_regressor__learning_rate=0.1, meta_regressor__booster=gblinear, total=   1.0s\n",
      "[CV] meta_regressor__n_estimators=300, meta_regressor__learning_rate=0.01, meta_regressor__booster=dart \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:33:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta_regressor__n_estimators=300, meta_regressor__learning_rate=0.01, meta_regressor__booster=dart, total=   1.1s\n",
      "[CV] meta_regressor__n_estimators=300, meta_regressor__learning_rate=0.01, meta_regressor__booster=dart \n",
      "[19:33:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  meta_regressor__n_estimators=300, meta_regressor__learning_rate=0.01, meta_regressor__booster=dart, total=   1.1s\n",
      "[CV] meta_regressor__n_estimators=300, meta_regressor__learning_rate=0.01, meta_regressor__booster=dart \n",
      "[19:34:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  meta_regressor__n_estimators=300, meta_regressor__learning_rate=0.01, meta_regressor__booster=dart, total=   1.0s\n",
      "[CV] meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.1, meta_regressor__booster=gblinear \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:34:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.1, meta_regressor__booster=gblinear, total=   1.0s\n",
      "[CV] meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.1, meta_regressor__booster=gblinear \n",
      "[19:34:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.1, meta_regressor__booster=gblinear, total=   0.9s\n",
      "[CV] meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.1, meta_regressor__booster=gblinear \n",
      "[19:34:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.1, meta_regressor__booster=gblinear, total=   1.0s\n",
      "[CV] meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.01, meta_regressor__booster=dart \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:34:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.01, meta_regressor__booster=dart, total=   1.1s\n",
      "[CV] meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.01, meta_regressor__booster=dart \n",
      "[19:34:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.01, meta_regressor__booster=dart, total=   1.0s\n",
      "[CV] meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.01, meta_regressor__booster=dart \n",
      "[19:34:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.01, meta_regressor__booster=dart, total=   1.0s\n",
      "[CV] meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.05, meta_regressor__booster=gbtree \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:34:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.05, meta_regressor__booster=gbtree, total=   1.0s\n",
      "[CV] meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.05, meta_regressor__booster=gbtree \n",
      "[19:34:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.05, meta_regressor__booster=gbtree, total=   1.0s\n",
      "[CV] meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.05, meta_regressor__booster=gbtree \n",
      "[19:34:09] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  meta_regressor__n_estimators=200, meta_regressor__learning_rate=0.05, meta_regressor__booster=gbtree, total=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:   30.8s finished\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:34:10] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Best Score:  -2286037.232217267\n",
      "Best Parameters: {'meta_regressor__n_estimators': 100, 'meta_regressor__learning_rate': 0.05, 'meta_regressor__booster': 'gblinear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning the stacked regressor - variant 1\n",
    "params = {\"meta_regressor__n_estimators\": [100, 200, 300],\n",
    "          \"meta_regressor__booster\": ['gbtree', 'gblinear', 'dart'],  \n",
    "          \"meta_regressor__learning_rate\": [0.05, 0.01, 0.1]}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "grid = RandomizedSearchCV(estimator = sreg1, param_distributions = params, cv = 3, scoring = \"neg_root_mean_squared_error\", verbose = 2)\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "grid.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Parameters and corresponding scores for random search\n",
    "print(\"Best Score: \", grid.best_score_)\n",
    "print(\"Best Parameters:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXtNkD5sy3qi",
    "outputId": "9243a93a-b0e4-4881-8060-8aea42b79a7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] meta_regressor__penalty=l2, meta_regressor__max_iter=5000, meta_regressor__learning_rate=optimal \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta_regressor__penalty=l2, meta_regressor__max_iter=5000, meta_regressor__learning_rate=optimal, total=   1.0s\n",
      "[CV] meta_regressor__penalty=l2, meta_regressor__max_iter=5000, meta_regressor__learning_rate=optimal \n",
      "[CV]  meta_regressor__penalty=l2, meta_regressor__max_iter=5000, meta_regressor__learning_rate=optimal, total=   1.0s\n",
      "[CV] meta_regressor__penalty=l2, meta_regressor__max_iter=5000, meta_regressor__learning_rate=optimal \n",
      "[CV]  meta_regressor__penalty=l2, meta_regressor__max_iter=5000, meta_regressor__learning_rate=optimal, total=   1.0s\n",
      "[CV] meta_regressor__penalty=l2, meta_regressor__max_iter=1000, meta_regressor__learning_rate=constant \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta_regressor__penalty=l2, meta_regressor__max_iter=1000, meta_regressor__learning_rate=constant, total=   1.0s\n",
      "[CV] meta_regressor__penalty=l2, meta_regressor__max_iter=1000, meta_regressor__learning_rate=constant \n",
      "[CV]  meta_regressor__penalty=l2, meta_regressor__max_iter=1000, meta_regressor__learning_rate=constant, total=   1.0s\n",
      "[CV] meta_regressor__penalty=l2, meta_regressor__max_iter=1000, meta_regressor__learning_rate=constant \n",
      "[CV]  meta_regressor__penalty=l2, meta_regressor__max_iter=1000, meta_regressor__learning_rate=constant, total=   0.9s\n",
      "[CV] meta_regressor__penalty=l2, meta_regressor__max_iter=1000, meta_regressor__learning_rate=optimal \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta_regressor__penalty=l2, meta_regressor__max_iter=1000, meta_regressor__learning_rate=optimal, total=   1.0s\n",
      "[CV] meta_regressor__penalty=l2, meta_regressor__max_iter=1000, meta_regressor__learning_rate=optimal \n",
      "[CV]  meta_regressor__penalty=l2, meta_regressor__max_iter=1000, meta_regressor__learning_rate=optimal, total=   1.0s\n",
      "[CV] meta_regressor__penalty=l2, meta_regressor__max_iter=1000, meta_regressor__learning_rate=optimal \n",
      "[CV]  meta_regressor__penalty=l2, meta_regressor__max_iter=1000, meta_regressor__learning_rate=optimal, total=   1.0s\n",
      "[CV] meta_regressor__penalty=l1, meta_regressor__max_iter=5000, meta_regressor__learning_rate=optimal \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta_regressor__penalty=l1, meta_regressor__max_iter=5000, meta_regressor__learning_rate=optimal, total=   1.0s\n",
      "[CV] meta_regressor__penalty=l1, meta_regressor__max_iter=5000, meta_regressor__learning_rate=optimal \n",
      "[CV]  meta_regressor__penalty=l1, meta_regressor__max_iter=5000, meta_regressor__learning_rate=optimal, total=   1.0s\n",
      "[CV] meta_regressor__penalty=l1, meta_regressor__max_iter=5000, meta_regressor__learning_rate=optimal \n",
      "[CV]  meta_regressor__penalty=l1, meta_regressor__max_iter=5000, meta_regressor__learning_rate=optimal, total=   1.0s\n",
      "[CV] meta_regressor__penalty=l2, meta_regressor__max_iter=2000, meta_regressor__learning_rate=optimal \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta_regressor__penalty=l2, meta_regressor__max_iter=2000, meta_regressor__learning_rate=optimal, total=   1.0s\n",
      "[CV] meta_regressor__penalty=l2, meta_regressor__max_iter=2000, meta_regressor__learning_rate=optimal \n",
      "[CV]  meta_regressor__penalty=l2, meta_regressor__max_iter=2000, meta_regressor__learning_rate=optimal, total=   1.0s\n",
      "[CV] meta_regressor__penalty=l2, meta_regressor__max_iter=2000, meta_regressor__learning_rate=optimal \n",
      "[CV]  meta_regressor__penalty=l2, meta_regressor__max_iter=2000, meta_regressor__learning_rate=optimal, total=   1.0s\n",
      "[CV] meta_regressor__penalty=elasticnet, meta_regressor__max_iter=3000, meta_regressor__learning_rate=invscaling \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta_regressor__penalty=elasticnet, meta_regressor__max_iter=3000, meta_regressor__learning_rate=invscaling, total=   1.0s\n",
      "[CV] meta_regressor__penalty=elasticnet, meta_regressor__max_iter=3000, meta_regressor__learning_rate=invscaling \n",
      "[CV]  meta_regressor__penalty=elasticnet, meta_regressor__max_iter=3000, meta_regressor__learning_rate=invscaling, total=   1.0s\n",
      "[CV] meta_regressor__penalty=elasticnet, meta_regressor__max_iter=3000, meta_regressor__learning_rate=invscaling \n",
      "[CV]  meta_regressor__penalty=elasticnet, meta_regressor__max_iter=3000, meta_regressor__learning_rate=invscaling, total=   0.9s\n",
      "[CV] meta_regressor__penalty=elasticnet, meta_regressor__max_iter=3000, meta_regressor__learning_rate=adaptive \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta_regressor__penalty=elasticnet, meta_regressor__max_iter=3000, meta_regressor__learning_rate=adaptive, total=   1.0s\n",
      "[CV] meta_regressor__penalty=elasticnet, meta_regressor__max_iter=3000, meta_regressor__learning_rate=adaptive \n",
      "[CV]  meta_regressor__penalty=elasticnet, meta_regressor__max_iter=3000, meta_regressor__learning_rate=adaptive, total=   1.0s\n",
      "[CV] meta_regressor__penalty=elasticnet, meta_regressor__max_iter=3000, meta_regressor__learning_rate=adaptive \n",
      "[CV]  meta_regressor__penalty=elasticnet, meta_regressor__max_iter=3000, meta_regressor__learning_rate=adaptive, total=   1.0s\n",
      "[CV] meta_regressor__penalty=elasticnet, meta_regressor__max_iter=5000, meta_regressor__learning_rate=optimal \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta_regressor__penalty=elasticnet, meta_regressor__max_iter=5000, meta_regressor__learning_rate=optimal, total=   1.0s\n",
      "[CV] meta_regressor__penalty=elasticnet, meta_regressor__max_iter=5000, meta_regressor__learning_rate=optimal \n",
      "[CV]  meta_regressor__penalty=elasticnet, meta_regressor__max_iter=5000, meta_regressor__learning_rate=optimal, total=   1.0s\n",
      "[CV] meta_regressor__penalty=elasticnet, meta_regressor__max_iter=5000, meta_regressor__learning_rate=optimal \n",
      "[CV]  meta_regressor__penalty=elasticnet, meta_regressor__max_iter=5000, meta_regressor__learning_rate=optimal, total=   0.9s\n",
      "[CV] meta_regressor__penalty=elasticnet, meta_regressor__max_iter=5000, meta_regressor__learning_rate=adaptive \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta_regressor__penalty=elasticnet, meta_regressor__max_iter=5000, meta_regressor__learning_rate=adaptive, total=   1.0s\n",
      "[CV] meta_regressor__penalty=elasticnet, meta_regressor__max_iter=5000, meta_regressor__learning_rate=adaptive \n",
      "[CV]  meta_regressor__penalty=elasticnet, meta_regressor__max_iter=5000, meta_regressor__learning_rate=adaptive, total=   1.0s\n",
      "[CV] meta_regressor__penalty=elasticnet, meta_regressor__max_iter=5000, meta_regressor__learning_rate=adaptive \n",
      "[CV]  meta_regressor__penalty=elasticnet, meta_regressor__max_iter=5000, meta_regressor__learning_rate=adaptive, total=   0.9s\n",
      "[CV] meta_regressor__penalty=l1, meta_regressor__max_iter=3000, meta_regressor__learning_rate=constant \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta_regressor__penalty=l1, meta_regressor__max_iter=3000, meta_regressor__learning_rate=constant, total=   1.0s\n",
      "[CV] meta_regressor__penalty=l1, meta_regressor__max_iter=3000, meta_regressor__learning_rate=constant \n",
      "[CV]  meta_regressor__penalty=l1, meta_regressor__max_iter=3000, meta_regressor__learning_rate=constant, total=   1.0s\n",
      "[CV] meta_regressor__penalty=l1, meta_regressor__max_iter=3000, meta_regressor__learning_rate=constant \n",
      "[CV]  meta_regressor__penalty=l1, meta_regressor__max_iter=3000, meta_regressor__learning_rate=constant, total=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:   29.5s finished\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  -1.5081945148013156e+19\n",
      "Best Parameters: {'meta_regressor__penalty': 'elasticnet', 'meta_regressor__max_iter': 5000, 'meta_regressor__learning_rate': 'adaptive'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning the stacked regressor - variant 2\n",
    "params = {\"meta_regressor__max_iter\": [1000, 2000, 3000, 5000],\n",
    "          \"meta_regressor__penalty\": ['l2', 'l1', 'elasticnet'],  \n",
    "          \"meta_regressor__learning_rate\": ['invscaling', 'constant', 'optimal', 'adaptive']}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "grid = RandomizedSearchCV(estimator = sreg2, param_distributions = params, cv = 3, scoring = \"neg_root_mean_squared_error\", verbose = 2)\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "grid.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Parameters and corresponding scores for random search\n",
    "print(\"Best Score: \", grid.best_score_)\n",
    "print(\"Best Parameters:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "cDcC5bAZs7hO"
   },
   "outputs": [],
   "source": [
    "base_models = [\n",
    "               DecisionTreeRegressor(splitter='best', min_weight_fraction_leaf=0.1, criterion='friedman_mse', random_state=0),\n",
    "               RandomForestRegressor(n_estimators=100, min_weight_fraction_leaf=0.05, criterion='mse', random_state=0),\n",
    "               MLPRegressor(learning_rate='adaptive', hidden_layer_sizes=(200, ), activation='relu', random_state=0, early_stopping=True),\n",
    "               SVR(kernel='linear', C=1.5),\n",
    "               SGDRegressor(penalty='l1', loss='huber', random_state=0)\n",
    "]\n",
    "\n",
    "# final stacked model - variant 1 \n",
    "\n",
    "meta_model3 = XGBRegressor(n_estimators=100, learning_rate=0.05, booster='gblinear')\n",
    "\n",
    "sreg3 = StackingCVRegressor(base_models,\n",
    "                           shuffle = False,\n",
    "                           cv = 3,\n",
    "                           meta_regressor = meta_model3)\n",
    "\n",
    "\n",
    "# final stacked model - variant 2\n",
    "\n",
    "meta_model4 = SGDRegressor(penalty='elasticnet', max_iter=5000, learning_rate='adaptive')\n",
    "\n",
    "sreg4 = StackingCVRegressor(base_models,\n",
    "                           shuffle = False,\n",
    "                           cv = 3,\n",
    "                           meta_regressor = meta_model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0-lNwDI3s-z8",
    "outputId": "84eb7789-f4e7-4a84-e41f-dcb499a0de91"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:39:18] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Model - Variant 1\n",
      "RMSE: 3334509.4471968007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Model - Variant 2\n",
      "RMSE: 2.8758910980933786e+18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# training the final stacked model - variant 1 \n",
    "sreg3.fit(X_train.values, y_train.values)\n",
    "y_pred3 = sreg3.predict(X_test)\n",
    "\n",
    "# generating rmse\n",
    "print('Stacked Model - Variant 1')\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred3, squared=False))\n",
    "\n",
    "# training the final stacked model - variant 2\n",
    "sreg4.fit(X_train.values, y_train.values)\n",
    "y_pred4 = sreg4.predict(X_test)\n",
    "\n",
    "# generating rmse\n",
    "print('Stacked Model - Variant 2')\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred4, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a5dTAJblYbm"
   },
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "InFkJW4zlZk_"
   },
   "outputs": [],
   "source": [
    "# creating copy of test file and removing unwanted from test data\n",
    "\n",
    "df2 = test_data.copy()\n",
    "id_col = df2.Id.copy()\n",
    "df2.drop(['Id', 'Open Date', 'City'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oVFwDOkbl1V7"
   },
   "outputs": [],
   "source": [
    "# preprocessing test data\n",
    "\n",
    "df2[cat_cols] = cat_imputer.transform(df2[cat_cols])\n",
    "\n",
    "df2[num_cols] = num_imputer.transform(df2[num_cols])\n",
    "\n",
    "df2 = pd.get_dummies(df2, columns=cat_cols, drop_first=True)\n",
    "\n",
    "df2[num_cols] = df2[num_cols].apply(lambda x: x.clip(*x.quantile([0.05, 0.95])))\n",
    "\n",
    "df2[num_cols] = scaler.fit_transform(df2[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "jg7MpW3JiMjp"
   },
   "outputs": [],
   "source": [
    "# generating predictions using base stacked model\n",
    "y_pred = sreg1.predict(df2.values)\n",
    "\n",
    "# generating submission dataset with QuoteNumber and QuoteConversion_Flag columns\n",
    "submit_df = pd.Series(y_pred, name='Prediction')\n",
    "submission = pd.concat([id_col, submit_df], axis=1)\n",
    "submission.to_csv('/content/drive/MyDrive/Colab Notebooks/TeamAssn/submit_sreg1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zGs6g34Elfoj"
   },
   "outputs": [],
   "source": [
    "# generating predictions using tuned stacked model\n",
    "y_pred = sreg3.predict(df2.values)\n",
    "\n",
    "# generating submission dataset with QuoteNumber and QuoteConversion_Flag columns\n",
    "submit_df = pd.Series(y_pred, name='Prediction')\n",
    "submission = pd.concat([id_col, submit_df], axis=1)\n",
    "submission.to_csv('/content/drive/MyDrive/Colab Notebooks/TeamAssn/submit_sreg3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "y_B9M8hFrO0C"
   },
   "outputs": [],
   "source": [
    "# generating predictions using best performing individual model\n",
    "reg = RandomForestRegressor(n_estimators=100, min_weight_fraction_leaf=0.05, criterion='mse', random_state=0)\n",
    "reg.fit(X, y)\n",
    "\n",
    "y_pred = reg.predict(df2.values)\n",
    "\n",
    "# generating submission dataset with QuoteNumber and QuoteConversion_Flag columns\n",
    "submit_df = pd.Series(y_pred, name='Prediction')\n",
    "submission = pd.concat([id_col, submit_df], axis=1)\n",
    "submission.to_csv('/content/drive/MyDrive/Colab Notebooks/TeamAssn/submit_reg.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "qaEcDsnzhxV1",
    "tmu2FqEpuHEj",
    "qym3W5zBht14",
    "G9F9kx__Y0Yp",
    "RLS3oS1EY4Um",
    "2a5dTAJblYbm"
   ],
   "name": "team_assn_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
